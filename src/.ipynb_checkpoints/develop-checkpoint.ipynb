{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tried implementing neural net learning based on 3blue1brown video and Nielssen book code\n",
    "## author: me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some data\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data), len(validation_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_training_data = training_data[:1000]\n",
    "red_validation_data = validation_data[:1000]\n",
    "red_test_data = test_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "testimage = training_data[0][0]\n",
    "testimage2 = training_data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create activation function\n",
    "def ReLU(x):\n",
    "    return max(0,x)\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid_prime(x):\n",
    "    return np.exp(-x)/(1+np.exp(-x))**2\n",
    "\n",
    "# create network\n",
    "class Network:\n",
    "    def __init__(self, layers,activation_function):\n",
    "        self.layers = np.append(784,layers)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.activations = []\n",
    "        self.zvalues = []\n",
    "        self.act_func = activation_function\n",
    "        #initialise weights and biases randomly\n",
    "        for i in range(len(self.layers)-1):\n",
    "            w = np.random.rand(self.layers[i+1],self.layers[i])\n",
    "            self.weights.append(w)\n",
    "            b = np.random.rand(self.layers[i+1])\n",
    "            self.biases.append(b)\n",
    "            z = np.zeros(self.layers[i+1])\n",
    "            self.zvalues.append(z)\n",
    "            a = np.zeros(self.layers[i+1])\n",
    "            self.activations.append(a)\n",
    "            \n",
    "    def forwardfeed(self,image):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            if i == 0:\n",
    "                a = np.dot(self.weights[i],image).flatten()\n",
    "                a = np.divide(a,len(image)) #normalize by number of terms in sum \n",
    "                b = self.biases[i]\n",
    "                self.activations[i] = [self.act_func(j) for j in np.add(a,b)]\n",
    "            else:\n",
    "                a = np.dot(self.weights[i],self.activations[i-1]).flatten()\n",
    "                a = np.divide(a,len(self.activations[i-1])) #normalize by number of terms in sum\n",
    "                b = self.biases[i]\n",
    "                self.activations[i] = [self.act_func(j) for j in np.add(a,b)]\n",
    "    \n",
    "    def cost(self):\n",
    "        costs = []\n",
    "        for i in red_training_data:\n",
    "            self.forwardfeed(i[0])\n",
    "            produced_activations = self.activations[-1]\n",
    "            cost = 0\n",
    "            for j in range(0,10):\n",
    "                desired_output = i[1]\n",
    "                cost += (produced_activations[j]-desired_output[j])**2\n",
    "            costs.append(cost)\n",
    "        return np.mean(costs)\n",
    "    \n",
    "    def train(self,trainingdata,batchsize=50,normfactor=100):\n",
    "        bs = batchsize\n",
    "        td = trainingdata\n",
    "        assert len(td)%bs == 0, \"trainingdata module batchsize non-zero\"\n",
    "        for i in range(len(td)/bs):\n",
    "            print(\"batch\",i)\n",
    "            training_batch = td[i*bs:(i+1)*bs]\n",
    "            weights_gradient_total = np.array([np.zeros((16,784)),np.zeros((18,16)),np.zeros((10,18))])\n",
    "            biases_gradient_total = np.array([np.zeros(16),np.zeros(18),np.zeros(10)])\n",
    "            for j in training_batch:\n",
    "                gradient = self.single_cost_gradient(j)\n",
    "                weights_gradient_total = np.add(weights_gradient_total,gradient[0])\n",
    "                biases_gradient_total = np.add(biases_gradient_total,gradient[1])\n",
    "            weights_gradient_total = np.divide(weights_gradient_total,bs*normfactor)\n",
    "            biases_gradient_total = np.divide(biases_gradient_total,bs*normfactor)\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] = np.add(self.weights[i],weights_gradient_total[i])\n",
    "                self.biases[i] = np.add(self.biases[i],biases_gradient_total[i])\n",
    "                \n",
    "     \n",
    "    def single_cost_gradient(self,training_sample):\n",
    "        ts = training_sample\n",
    "        weight_nudges = np.array([np.zeros((16,784)),np.zeros((18,16)),np.zeros((10,18))])\n",
    "        biases_nudges = np.array([np.zeros(16),np.zeros(18),np.zeros(10)])\n",
    "\n",
    "        for j in range(self.layers[-1]): #L layer\n",
    "            dc_daj = 2*(self.activations[-1][j]-ts[1][j])\n",
    "            daj_dzj = sigmoid_prime(self.zvalues[-1][j])\n",
    "            biases_nudges[-1][j] = dc_daj*daj_dzj   #L layer biases\n",
    "\n",
    "            derivative_sum = 0\n",
    "            for k in range(self.layers[-2]): #L-1 layer\n",
    "                dzj_dwjk = self.activations[-2][k]\n",
    "                weight_nudges[-1][j,k] = dc_daj*daj_dzj*dzj_dwjk #L layer weights\n",
    "\n",
    "                dzj_dzk = sigmoid_prime(self.zvalues[-2][k])\n",
    "                derivative_sum += dc_daj*daj_dzj*dzj_dzk\n",
    "\n",
    "                biases_nudges[-2][k] = derivative_sum  #L-1 layer biases\n",
    "                derivative_sum2 = 0\n",
    "                for l in range(self.layers[-3]): #L-2 layer\n",
    "                    dzk_dwkl = self.activations[-3][l]\n",
    "                    weight_nudges[-2][k,l] = derivative_sum*dzk_dwkl #L-1 layer weights\n",
    "\n",
    "                    dzk_dzl = sigmoid_prime(self.zvalues[-3][l])\n",
    "                    derivative_sum2 += derivative_sum*dzk_dzl\n",
    "\n",
    "                    biases_nudges[-3][l] = derivative_sum2 #L-2 layer biases\n",
    "                    for h in range(self.layers[-4]): #L-3 layer\n",
    "                        dzl_dwlm = ts[0][h]\n",
    "                        weight_nudges[-3][l,h] = derivative_sum2*dzl_dwlm #L-2 layer weights\n",
    "\n",
    "        return weight_nudges, biases_nudges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "testnet = Network([16,18,10],sigmoid)\n",
    "#testnet2 = Network([16,18,10],sigmoid) #different due to random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testnet.forwardfeed(testimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.90101668,  0.6352517 ,  0.90695253,  0.7364634 ,  0.87976872,\n",
       "        0.0736008 ,  0.68225777,  0.97853571,  0.27787319,  0.59951349,\n",
       "        0.34411534,  0.44010823,  0.83983052,  0.23792687,  0.50625824,\n",
       "        0.67196369,  0.13451844,  0.56851194])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testnet.weights[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xx = testnet.single_cost_gradient(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('batch', 0)\n",
      "('batch', 1)\n",
      "('batch', 2)\n",
      "('batch', 3)\n",
      "('batch', 4)\n",
      "('batch', 5)\n",
      "('batch', 6)\n",
      "('batch', 7)\n",
      "('batch', 8)\n",
      "('batch', 9)\n"
     ]
    }
   ],
   "source": [
    "testnet.train(training_data[:50],batchsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.92371709,  0.66031504,  0.93011888,  0.76127519,  0.90333048,\n",
       "        0.0991539 ,  0.70800024,  1.00172473,  0.30068109,  0.62141756,\n",
       "        0.36917503,  0.46314131,  0.86004338,  0.26179935,  0.52823281,\n",
       "        0.69579357,  0.15706414,  0.59443626])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testnet.weights[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9248293 ,  0.612632  ,  0.14186262,  0.94156069,  0.91484248,\n",
       "        0.90670774,  0.86404458,  0.52250647,  0.37142391,  0.47308001])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testnet.biases[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.78185599031696607,\n",
       " 0.72223413594703767,\n",
       " 0.60840628395109531,\n",
       " 0.7823576194808225,\n",
       " 0.77153751174259055,\n",
       " 0.77121931772663777,\n",
       " 0.75271893508579268,\n",
       " 0.69471531072056958,\n",
       " 0.66979123067031798,\n",
       " 0.67860423150551152]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testnet.activations[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "testnet.forwardfeed(testimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.79280143955762594,\n",
       " 0.73366926629805329,\n",
       " 0.62065530991928342,\n",
       " 0.79266229277139966,\n",
       " 0.78256973338797731,\n",
       " 0.78261638386403309,\n",
       " 0.76345490820405992,\n",
       " 0.70687120547424787,\n",
       " 0.68273757841818861,\n",
       " 0.68978670716227963]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testnet.activations[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.16 64-bit",
   "language": "python",
   "name": "python271664bit9e0270b35be64079b3cfbcc6e38f6389"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
